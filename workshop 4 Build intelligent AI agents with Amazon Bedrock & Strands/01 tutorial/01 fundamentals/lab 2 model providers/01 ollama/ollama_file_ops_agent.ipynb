{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Local Agent with Strands Agents and Ollama Model\n",
    "\n",
    "This notebook demonstrates how to create a personal agent using Strands Agent and Ollama. The agent will be capable of performing various local tasks such as file operations, web searches, and system commands.\n",
    "\n",
    "## What is Ollama?\n",
    "\n",
    "[Ollama](https://ollama.com/) is an open-source framework that allows you to run large language models (LLMs) locally on your machine. It provides a simple API for interacting with these models, making it ideal for privacy-focused applications where you don't want to send data to external services.\n",
    "\n",
    "Key benefits of Ollama:\n",
    "- **Privacy**: All processing happens locally on your machine\n",
    "- **No API costs**: Free to use as much as you want\n",
    "- **Offline capability**: Works without internet connection\n",
    "- **Customization**: Can be fine-tuned for specific use \n",
    "\n",
    "\n",
    "## Agent Details\n",
    "\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Feature used        |Ollama Model - to create a file operations agent   |\n",
    "|Agent Structure     |Single agent architecture                          |\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: strands-agents in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.20.0)\n",
      "Requirement already satisfied: strands-agents-tools in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.18)\n",
      "Requirement already satisfied: PyPDF2>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.42.12)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.42.12)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (0.60b1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (1.39.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (2.12.5)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (2.6.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->strands-agents->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->strands-agents->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->strands-agents->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->strands-agents->-r requirements.txt (line 1)) (0.30.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (4.12.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: httpx>=0.27.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.0.21)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.50.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.38.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.60b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.60b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.60b1->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (0.60b1)\n",
      "Requirement already satisfied: packaging>=18.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.60b1->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents->-r requirements.txt (line 1)) (2.41.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: aws-requests-auth<0.5.0,>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: markdownify<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.2.2)\n",
      "Collecting pillow<12.0.0,>=11.2.1 (from strands-agents-tools->-r requirements.txt (line 2))\n",
      "  Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (3.0.52)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (14.2.0)\n",
      "Requirement already satisfied: slack-bolt<2.0.0,>=1.23.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools->-r requirements.txt (line 2)) (9.1.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.0->strands-agents-tools->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /opt/conda/lib/python3.12/site-packages (from markdownify<2.0.0,>=1.0.0->strands-agents-tools->-r requirements.txt (line 2)) (4.14.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.9->markdownify<2.0.0,>=1.0.0->strands-agents-tools->-r requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools->-r requirements.txt (line 2)) (0.2.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.0->strands-agents-tools->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: slack_sdk<4,>=3.35.0 in /opt/conda/lib/python3.12/site-packages (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools->-r requirements.txt (line 2)) (3.37.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.4.8 in /opt/conda/lib/python3.12/site-packages (from strands-agents[ollama]->-r requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (2.23)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.11.0->strands-agents->-r requirements.txt (line 1)) (8.3.1)\n",
      "Using cached pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 12.0.0\n",
      "    Uninstalling pillow-12.0.0:\n",
      "      Successfully uninstalled pillow-12.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "autogluon-multimodal 1.4.0 requires fsspec[http]<=2025.3, but you have fsspec 2025.10.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires jsonschema<4.24,>=4.18, but you have jsonschema 4.25.1 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.1 which is incompatible.\n",
      "mlflow 2.22.0 requires packaging<25, but you have packaging 25.0 which is incompatible.\n",
      "mlflow 2.22.0 requires pyarrow<20,>=4.0.0, but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pillow-11.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "\n",
    "1. Install Ollama: Follow instructions at [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Start the Ollama server: `ollama serve`\n",
    "3. Downloaded a model with Ollama: `ollama pull llama3.2:1b`\n",
    "\n",
    "Refer to the [documentation](https://cuddly-sniffle-lrmk2y7.pages.github.io/0.1.x/user-guide/concepts/model-providers/ollama/) for detailed instructions.\n",
    "\n",
    "In this notebook, we will download Ollama for the linux distribution for compatibility with SageMaker Studio. This is done for code execution during AWS lead workshops on Workshop Studio. If you are running this code locally, you should adjust the steps to download Ollama to your current enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# this will work on linux computers\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['ollama', 'serve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/12/19 - 04:56:04 | 200 |    2.267875ms |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l[GIN] 2025/12/19 - 04:56:05 | 200 |  529.146677ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# Import strands components\n",
    "from strands import Agent, tool\n",
    "from strands.models.ollama import OllamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/12/19 - 04:56:19 | 200 |     597.201µs |       127.0.0.1 | GET      \"/api/tags\"\n",
      "✅ Ollama is running. Available models:\n",
      "- llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running:\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    print(\"✅ Ollama is running. Available models:\")\n",
    "    for model in response.json().get(\"models\", []):\n",
    "        print(f\"- {model['name']}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Ollama is not running. Please start Ollama before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Tools\n",
    "\n",
    "Tools are functions that the agent can use to interact with the environment. Below, we define several useful tools for our personal agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Operation Tools\n",
    "@tool\n",
    "def file_read(file_path: str) -> str:\n",
    "    \"\"\"Read a file and return its content. Supports both text and PDF files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to read\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the file\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if it's a PDF file\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            import PyPDF2\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text if text.strip() else \"Error: Could not extract text from PDF\"\n",
    "        else:\n",
    "            # Regular text file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File '{file_path}' not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def file_write(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write content to a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file\n",
    "        content (str): The content to write to the file\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(content)\n",
    "        return f\"File '{file_path}' written successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_directory(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List files and directories in the specified path.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to list\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string listing all files and directories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        items = os.listdir(directory_path)\n",
    "        files = []\n",
    "        directories = []\n",
    "\n",
    "        for item in items:\n",
    "            full_path = os.path.join(directory_path, item)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(f\"Folder: {item}/\")\n",
    "            else:\n",
    "                files.append(f\"File: {item}\")\n",
    "\n",
    "        result = f\"Contents of {os.path.abspath(directory_path)}:\\n\"\n",
    "        result += (\n",
    "            \"\\nDirectories:\\n\" + \"\\n\".join(sorted(directories))\n",
    "            if directories\n",
    "            else \"\\nNo directories found.\"\n",
    "        )\n",
    "        result += (\n",
    "            \"\\n\\nFiles:\\n\" + \"\\n\".join(sorted(files)) if files else \"\\nNo files found.\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error listing directory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Ollama-powered Agent\n",
    "\n",
    "Now we'll create our agent using the Ollama model and the tools we defined above.\n",
    "\n",
    "Note: You can add more tools like `execute_commands`, `search_files` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive system prompt for our agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful personal assistant capable of performing local file actions and simple tasks for the user.\n",
    "\n",
    "Your key capabilities:\n",
    "1. Read, understand, and summarize files.\n",
    "2. Create and write to files.\n",
    "3. List directory contents and provide information on the files.\n",
    "4. Summarize text content\n",
    "\n",
    "When using tools:\n",
    "- Always verify file paths before operations\n",
    "- Be careful with system commands\n",
    "- Provide clear explanations of what you're doing\n",
    "- If a task cannot be completed, explain why and suggest alternatives\n",
    "\n",
    "Always be helpful, concise, and focus on addressing the user's needs efficiently.\n",
    "\"\"\"\n",
    "\n",
    "model_id = (\n",
    "    \"llama3.2:3b\"  # You can change this to any model you have pulled with Ollama.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the Ollama model\n",
    "Make sure your Ollama service is running at http://localhost:11434 and your `model_id` is in the list of Ollama models printed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaModel(\n",
    "    model_id=model_id,\n",
    "    host=\"http://localhost:11434\",\n",
    "    max_tokens=4096,  # Adjust based on your model's capabilities\n",
    "    temperature=0.7,  # Lower for more deterministic responses, higher for more creative\n",
    "    top_p=0.9,  # Nucleus sampling parameter\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "local_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    model=ollama_model,\n",
    "    tools=[file_read, file_write, list_directory],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent\n",
    "\n",
    "Let's test our agent with some example tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-12-19T04:56:57.631Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-12-19T04:56:58.088Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --model /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 45445\"\n",
      "time=2025-12-19T04:56:58.088Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"30.6 GiB\" free=\"19.1 GiB\" free_swap=\"0 B\"\n",
      "time=2025-12-19T04:56:58.088Z level=INFO source=server.go:496 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-12-19T04:56:58.089Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.9 GiB\"\n",
      "time=2025-12-19T04:56:58.089Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2025-12-19T04:56:58.089Z level=INFO source=device.go:272 msg=\"total memory\" size=\"2.3 GiB\"\n",
      "time=2025-12-19T04:56:58.102Z level=INFO source=runner.go:965 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-12-19T04:56:58.111Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2025-12-19T04:56:58.111Z level=INFO source=runner.go:1001 msg=\"Server listening on 127.0.0.1:45445\"\n",
      "time=2025-12-19T04:56:58.121Z level=INFO source=runner.go:895 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-19T04:56:58.121Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-19T04:56:58.122Z level=INFO source=server.go:1372 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: no_alloc         = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_embd_inp       = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: n_expert_groups  = 0\n",
      "print_info: n_group_used     = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_yarn_log_mul= 0.0000\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
      "load_tensors:          CPU model buffer size =  1918.35 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_seq     = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = auto\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.50 MiB\n",
      "llama_kv_cache:        CPU KV buffer size =   448.00 MiB\n",
      "llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_context: Flash Attention was auto, set to enabled\n",
      "llama_context:        CPU compute buffer size =   256.50 MiB\n",
      "llama_context: graph nodes  = 875\n",
      "llama_context: graph splits = 1\n",
      "time=2025-12-19T04:57:00.128Z level=INFO source=server.go:1376 msg=\"llama runner started in 2.04 seconds\"\n",
      "time=2025-12-19T04:57:00.128Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-19T04:57:00.128Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-19T04:57:00.128Z level=INFO source=server.go:1376 msg=\"llama runner started in 2.04 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: file_read\n",
      "[GIN] 2025/12/19 - 04:57:29 | 200 | 32.047027062s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-12-19T04:57:29.906Z level=WARN source=runner.go:153 msg=\"truncating input prompt\" limit=4096 prompt=9380 keep=5 new=4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text consists of two sections: the first is an excerpt from a letter written by Jeff Bezos, the founder and CEO of Amazon, in 1997. The second section appears to be a copy of Amazon's annual report for 1997.\n",
      "\n",
      "**Excerpt from Letter (1997)**\n",
      "\n",
      "This section provides an overview of Amazon's progress in 1997, highlighting its customer growth, revenue increase, and expansion of its business infrastructure.\n",
      "\n",
      "*   **Sales Growth**: Amazon's sales grew from $15.7 million in 1996 to $147.8 million in 1997, representing an 838% increase.\n",
      "*   **Customer Growth**: The company's cumulative customer accounts increased from 180,000 to 1,510,000 between May and December 1996, a growth rate of 738%.\n",
      "*   **Repeat Business**: Amazon reported that over 58% of orders in the fourth quarter of 1997 came from repeat customers.\n",
      "*   **Web Traffic**: The company's website ranked within the top 20 according to Media Metrix, indicating significant growth in audience reach.\n",
      "\n",
      "**Amazon's Annual Report (1997)**\n",
      "\n",
      "This section provides a comprehensive overview of Amazon's business operations, financial performance, and strategic objectives for 1997.\n",
      "\n",
      "*   **Financial Highlights**: Amazon reported net income of $0.02 per share, with revenue exceeding $147.8 million.\n",
      "*   **Employees and Management**: The company had 614 employees in 1997, with a strong management team in place to oversee its operations.\n",
      "*   **Infrastructure Expansion**: Amazon expanded its distribution center capacity by 70% and launched a second facility in Delaware, enabling the company to improve customer service and availability.\n",
      "*   **Strategic Objectives**: The company emphasized its focus on customer satisfaction, investing heavily in systems and infrastructure to support online commerce and expand its product offerings.\n",
      "\n",
      "**Letter from Jeff Bezos (1997)**\n",
      "\n",
      "This section concludes with a statement from Bezos, highlighting Amazon's commitment to long-term success and growth. He emphasizes the importance of prioritizing investments and maintaining a sense of urgency as the company navigates challenges in the competitive online marketplace.\n",
      "\n",
      "*   **Long-Term Focus**: Bezos reiterated Amazon's focus on creating value for customers through innovative products and services.\n",
      "*   **Execution Risk**: The company acknowledged the risks associated with product and geographic expansion, but remained optimistic about its long-term prospects.\n",
      "*   **Shareholder Support**: Bezos expressed gratitude to shareholders for their support and encouragement.\n",
      "\n",
      "**Goals for 1998**\n",
      "\n",
      "This section outlines Amazon's strategic objectives for 1998, including:\n",
      "\n",
      "*   **Investment in Systems and Infrastructure**: The company planned to continue investing in systems and infrastructure to support customer convenience, selection, and service.\n",
      "*   **Expansion into New Markets**: Amazon aimed to better serve customers overseas by reducing delivery times and tailoring the customer experience.\n",
      "*   **Prioritization of Investments**: The company emphasized the importance of prioritizing investments and maintaining a sense of urgency as it navigated challenges in the competitive online marketplace.[GIN] 2025/12/19 - 05:04:16 | 200 |         6m46s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': \"The text consists of two sections: the first is an excerpt from a letter written by Jeff Bezos, the founder and CEO of Amazon, in 1997. The second section appears to be a copy of Amazon's annual report for 1997.\\n\\n**Excerpt from Letter (1997)**\\n\\nThis section provides an overview of Amazon's progress in 1997, highlighting its customer growth, revenue increase, and expansion of its business infrastructure.\\n\\n*   **Sales Growth**: Amazon's sales grew from $15.7 million in 1996 to $147.8 million in 1997, representing an 838% increase.\\n*   **Customer Growth**: The company's cumulative customer accounts increased from 180,000 to 1,510,000 between May and December 1996, a growth rate of 738%.\\n*   **Repeat Business**: Amazon reported that over 58% of orders in the fourth quarter of 1997 came from repeat customers.\\n*   **Web Traffic**: The company's website ranked within the top 20 according to Media Metrix, indicating significant growth in audience reach.\\n\\n**Amazon's Annual Report (1997)**\\n\\nThis section provides a comprehensive overview of Amazon's business operations, financial performance, and strategic objectives for 1997.\\n\\n*   **Financial Highlights**: Amazon reported net income of $0.02 per share, with revenue exceeding $147.8 million.\\n*   **Employees and Management**: The company had 614 employees in 1997, with a strong management team in place to oversee its operations.\\n*   **Infrastructure Expansion**: Amazon expanded its distribution center capacity by 70% and launched a second facility in Delaware, enabling the company to improve customer service and availability.\\n*   **Strategic Objectives**: The company emphasized its focus on customer satisfaction, investing heavily in systems and infrastructure to support online commerce and expand its product offerings.\\n\\n**Letter from Jeff Bezos (1997)**\\n\\nThis section concludes with a statement from Bezos, highlighting Amazon's commitment to long-term success and growth. He emphasizes the importance of prioritizing investments and maintaining a sense of urgency as the company navigates challenges in the competitive online marketplace.\\n\\n*   **Long-Term Focus**: Bezos reiterated Amazon's focus on creating value for customers through innovative products and services.\\n*   **Execution Risk**: The company acknowledged the risks associated with product and geographic expansion, but remained optimistic about its long-term prospects.\\n*   **Shareholder Support**: Bezos expressed gratitude to shareholders for their support and encouragement.\\n\\n**Goals for 1998**\\n\\nThis section outlines Amazon's strategic objectives for 1998, including:\\n\\n*   **Investment in Systems and Infrastructure**: The company planned to continue investing in systems and infrastructure to support customer convenience, selection, and service.\\n*   **Expansion into New Markets**: Amazon aimed to better serve customers overseas by reducing delivery times and tailoring the customer experience.\\n*   **Prioritization of Investments**: The company emphasized the importance of prioritizing investments and maintaining a sense of urgency as it navigated challenges in the competitive online marketplace.\"}]}, metrics=EventLoopMetrics(cycle_count=2, tool_metrics={'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf'}}, call_count=1, success_count=1, error_count=0, total_time=0.18240094184875488)}, cycle_durations=[406.4488523006439], traces=[<strands.telemetry.metrics.Trace object at 0x7f48aa1c7680>, <strands.telemetry.metrics.Trace object at 0x7f48ba45e6f0>], accumulated_usage={'inputTokens': 657, 'outputTokens': 4656, 'totalTokens': 5313}, accumulated_metrics={'latencyMs': 438491.83014}), state={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_agent(\n",
    "    \"Read the file in the path `sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf` and summarize it in 5 bullet points.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-12-19T05:12:06.114Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-12-19T05:12:06.561Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --model /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 33315\"\n",
      "time=2025-12-19T05:12:06.562Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"30.6 GiB\" free=\"19.7 GiB\" free_swap=\"0 B\"\n",
      "time=2025-12-19T05:12:06.562Z level=INFO source=server.go:496 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-12-19T05:12:06.562Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"1.9 GiB\"\n",
      "time=2025-12-19T05:12:06.562Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2025-12-19T05:12:06.562Z level=INFO source=device.go:272 msg=\"total memory\" size=\"2.3 GiB\"\n",
      "time=2025-12-19T05:12:06.576Z level=INFO source=runner.go:965 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-12-19T05:12:06.585Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2025-12-19T05:12:06.585Z level=INFO source=runner.go:1001 msg=\"Server listening on 127.0.0.1:33315\"\n",
      "time=2025-12-19T05:12:06.595Z level=INFO source=runner.go:895 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Auto KvSize:4096 KvCacheType: NumThreads:4 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-12-19T05:12:06.595Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-19T05:12:06.595Z level=INFO source=server.go:1372 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /home/sagemaker-user/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128008 ('<|eom_id|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: no_alloc         = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_embd_inp       = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: n_expert_groups  = 0\n",
      "print_info: n_group_used     = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_yarn_log_mul= 0.0000\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
      "load_tensors:          CPU model buffer size =  1918.35 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_seq     = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = auto\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.50 MiB\n",
      "llama_kv_cache:        CPU KV buffer size =   448.00 MiB\n",
      "llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
      "llama_context: Flash Attention was auto, set to enabled\n",
      "llama_context:        CPU compute buffer size =   256.50 MiB\n",
      "llama_context: graph nodes  = 875\n",
      "llama_context: graph splits = 1\n",
      "time=2025-12-19T05:12:08.602Z level=INFO source=server.go:1376 msg=\"llama runner started in 2.04 seconds\"\n",
      "time=2025-12-19T05:12:08.602Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2025-12-19T05:12:08.602Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-12-19T05:12:08.603Z level=INFO source=server.go:1376 msg=\"llama runner started in 2.04 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #2: list_directory\n",
      "[GIN] 2025/12/19 - 05:13:09 | 200 |          1m3s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Here is a list of files in the current directory:\n",
      "\n",
      "1. `requirements.txt`\n",
      "2. `sample_file.txt`[GIN] 2025/12/19 - 05:13:19 | 200 |  9.732682806s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "# Example 2: List files in the current directory\n",
    "response = local_agent(\"Show me the files in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #3: file_write\n",
      "\n",
      "Tool #4: file_write\n",
      "[GIN] 2025/12/19 - 05:14:38 | 200 | 31.886541693s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "\n",
      "Tool #5: list_directory\n",
      "[GIN] 2025/12/19 - 05:14:47 | 200 |  8.998909996s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Here is the updated list of files in the current directory:\n",
      "\n",
      "1. `sample1.txt`\n",
      "2. `sample.txt`[GIN] 2025/12/19 - 05:14:56 | 200 |  8.657446069s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Create a sample file\n",
    "response = local_agent(\n",
    "    \"Create a file called 'sample.txt' with the content 'This is a test file created by my Ollama agent.'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #6: list_directory\n",
      "[GIN] 2025/12/19 - 05:17:48 | 200 | 25.803793792s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "Since there is no `.git` directory, I will create a README.md file instead.\n",
      "\n",
      "# Ollama Model Provider\n",
      "\n",
      "This directory contains the basic structure and files needed to get started with an Ollama model provider. It includes example code and sample files to demonstrate how to use the model provider.\n",
      "\n",
      "## Usage\n",
      "\n",
      "To use this model provider, simply run the `ollama_file_ops_agent.ipynb` file using your preferred Jupyter notebook or Python IDE.\n",
      "\n",
      "## Files\n",
      "\n",
      "*   `requirements.txt`: List of dependencies required to run the model provider.\n",
      "*   `sample1.txt` and `sample.txt`: Sample files used to demonstrate the functionality of the model provider.\n",
      "*   `ollama_file_ops_agent.ipynb`: The main code file that provides the implementation for the Ollama model provider.\n",
      "\n",
      "## Directory Structure\n",
      "\n",
      "The directory structure is as follows:\n",
      "```\n",
      ".\n",
      "├── README.md\n",
      "├── ollama_file_ops_agent.ipynb\n",
      "└── requirements.txt\n",
      "```[GIN] 2025/12/19 - 05:18:20 | 200 | 31.794622284s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentResult(stop_reason='end_turn', message={'role': 'assistant', 'content': [{'text': 'Since there is no `.git` directory, I will create a README.md file instead.\\n\\n# Ollama Model Provider\\n\\nThis directory contains the basic structure and files needed to get started with an Ollama model provider. It includes example code and sample files to demonstrate how to use the model provider.\\n\\n## Usage\\n\\nTo use this model provider, simply run the `ollama_file_ops_agent.ipynb` file using your preferred Jupyter notebook or Python IDE.\\n\\n## Files\\n\\n*   `requirements.txt`: List of dependencies required to run the model provider.\\n*   `sample1.txt` and `sample.txt`: Sample files used to demonstrate the functionality of the model provider.\\n*   `ollama_file_ops_agent.ipynb`: The main code file that provides the implementation for the Ollama model provider.\\n\\n## Directory Structure\\n\\nThe directory structure is as follows:\\n```\\n.\\n├── README.md\\n├── ollama_file_ops_agent.ipynb\\n└── requirements.txt\\n```'}]}, metrics=EventLoopMetrics(cycle_count=12, tool_metrics={'file_read': ToolMetrics(tool={'toolUseId': 'file_read', 'name': 'file_read', 'input': {'file_path': 'sample_file/Amazon-com-Inc-2023-Shareholder-Letter.pdf'}}, call_count=1, success_count=1, error_count=0, total_time=0.18240094184875488), 'list_directory': ToolMetrics(tool={'toolUseId': 'list_directory', 'name': 'list_directory', 'input': {'directory_path': '.git'}}, call_count=3, success_count=3, error_count=0, total_time=0.0013344287872314453), 'file_write': ToolMetrics(tool={'toolUseId': 'file_write', 'name': 'file_write', 'input': {'content': 'This is a test file created by my Ollama agent.', 'file_path': 'sample.txt'}}, call_count=2, success_count=2, error_count=0, total_time=0.002003192901611328)}, cycle_durations=[406.4488523006439, 9.736496925354004, 8.661372900009155, 31.798893690109253], traces=[<strands.telemetry.metrics.Trace object at 0x7f48aa1c7680>, <strands.telemetry.metrics.Trace object at 0x7f48ba45e6f0>, <strands.telemetry.metrics.Trace object at 0x7f48aa1d9490>, <strands.telemetry.metrics.Trace object at 0x7f48b9996cc0>, <strands.telemetry.metrics.Trace object at 0x7f48aa1da570>, <strands.telemetry.metrics.Trace object at 0x7f48aa1e50d0>, <strands.telemetry.metrics.Trace object at 0x7f48b9996d50>, <strands.telemetry.metrics.Trace object at 0x7f48b9666090>, <strands.telemetry.metrics.Trace object at 0x7f48b9666030>, <strands.telemetry.metrics.Trace object at 0x7f48aa1c6b10>, <strands.telemetry.metrics.Trace object at 0x7f48b93c4440>, <strands.telemetry.metrics.Trace object at 0x7f48b9659df0>], accumulated_usage={'inputTokens': 1023, 'outputTokens': 13231, 'totalTokens': 14254}, accumulated_metrics={'latencyMs': 618956.6515869999}), state={})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4: create a readme file after reading and understanding multiple files\n",
    "#response = local_agent(\"Create a readme.md for the current directory\")\n",
    "response = await local_agent.invoke_async(\n",
    "    \"Create a readme.md for the current directory\"\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've created a local personal agent using Stands and Ollama. The agent can perform file operations (read, write, append) and Summarize/Analyze text\n",
    "\n",
    "This demonstrates the power of running AI models locally with Ollama, combined with the flexibility of strands' tool system. You can extend this agent by adding more tools or using different Ollama models based on your needs.\n",
    "\n",
    "### Next Steps (Ideas)\n",
    "\n",
    "- Try different Ollama models to see how they affect the agent's capabilities\n",
    "- Add more complex tools like web search, email sending, or calendar integration\n",
    "- Implement memory for the agent to remember past interactions\n",
    "- Create a simple UI for interacting with your agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
